{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Aula5_ex_2.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"o_LuhjB6zXCy","colab_type":"text"},"source":["NLP Pipeline - Parte 1"]},{"cell_type":"code","metadata":{"id":"xenH7cBDzV30","colab_type":"code","colab":{}},"source":["import nltk\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KkbsXrpAz4xv","colab_type":"text"},"source":["Tokenização é o processo de dividir uma string em uma lista de palavras e pontuação."]},{"cell_type":"code","metadata":{"id":"7Hs6OZY7z5m9","colab_type":"code","colab":{}},"source":["my_string = \"I am learning Natural Language Processing.\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Um7YxtT8z_hR","colab_type":"code","outputId":"24ac32e0-ffb5-4ad6-d94d-fa884d12d5b1","executionInfo":{"status":"ok","timestamp":1576352873773,"user_tz":180,"elapsed":665,"user":{"displayName":"Wellington Franco","photoUrl":"","userId":"16483586833616386684"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["my_string"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'I am learning Natural Language Processing.'"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"g8RY4h-n0DxZ","colab_type":"text"},"source":["Para Tokenizar"]},{"cell_type":"code","metadata":{"id":"Vlb3wKHw0OHW","colab_type":"code","outputId":"536936cc-fc90-46aa-9763-f4bef7745384","executionInfo":{"status":"ok","timestamp":1576352921299,"user_tz":180,"elapsed":1244,"user":{"displayName":"Wellington Franco","photoUrl":"","userId":"16483586833616386684"}},"colab":{"base_uri":"https://localhost:8080/","height":70}},"source":["nltk.download('punkt')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"EgO1D7NE0GPi","colab_type":"code","colab":{}},"source":["tokens = nltk.word_tokenize(my_string)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"quXj1NlU0Q8w","colab_type":"code","outputId":"cac3efa1-d0e4-427a-a343-ab889bfa71e5","executionInfo":{"status":"ok","timestamp":1576353039777,"user_tz":180,"elapsed":755,"user":{"displayName":"Wellington Franco","photoUrl":"","userId":"16483586833616386684"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["tokens"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['I', 'am', 'learning', 'Natural', 'Language', 'Processing', '.']"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"ZhzeiiyD1GSA","colab_type":"code","outputId":"178a1502-039c-44c4-e35a-4797c562b90a","executionInfo":{"status":"ok","timestamp":1576353130882,"user_tz":180,"elapsed":681,"user":{"displayName":"Wellington Franco","photoUrl":"","userId":"16483586833616386684"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["len(tokens)"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["7"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"pdL3KUSW1Hvx","colab_type":"code","colab":{}},"source":["phrase = \"I am learning Natural Language Processing. \n","I am learning how to tokenize!\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hVXlenbI1MYS","colab_type":"code","colab":{}},"source":["tokens_sent = nltk.sent_tokenize(phrase)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eigEI7cf1SM9","colab_type":"code","outputId":"75bf8f82-c189-40bc-a806-e4c944dd31cd","executionInfo":{"status":"ok","timestamp":1576317303000,"user_tz":180,"elapsed":711,"user":{"displayName":"Wellington Franco","photoUrl":"","userId":"16483586833616386684"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["tokens_sent"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['I am learning Natural Language Processing.',\n"," 'I am learning how to tokenize!']"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"KHVqYofO1UXe","colab_type":"code","outputId":"1002ecc6-14df-4431-ee14-2516e8000820","executionInfo":{"status":"ok","timestamp":1576317344561,"user_tz":180,"elapsed":799,"user":{"displayName":"Wellington Franco","photoUrl":"","userId":"16483586833616386684"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["len(tokens_sent)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"xg6R4oqa1jMl","colab_type":"text"},"source":["Podemos tokenizar nossos tokens de sentença."]},{"cell_type":"code","metadata":{"id":"436LRUR_1j_W","colab_type":"code","outputId":"c267ec5c-6604-48b7-ec6d-a914229d9489","executionInfo":{"status":"ok","timestamp":1576317380724,"user_tz":180,"elapsed":684,"user":{"displayName":"Wellington Franco","photoUrl":"","userId":"16483586833616386684"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["for item in tokens_sent:\n","    print(nltk.word_tokenize(item))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["['I', 'am', 'learning', 'Natural', 'Language', 'Processing', '.']\n","['I', 'am', 'learning', 'how', 'to', 'tokenize', '!']\n"],"name":"stdout"}]}]}